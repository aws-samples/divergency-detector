1
00:00:00,110 --> 00:00:02,840
Introduction to an AWS Glue Crawler.

2
00:00:03,870 --> 00:00:06,460
Let's start with some key concepts.

3
00:00:06,480 --> 00:00:09,120
What is an AWS Glue crawler?

4
00:00:09,280 --> 00:00:11,120
An AWS Glue crawler

5
00:00:11,430 --> 00:00:14,600
is a program that connects to a datastore,

6
00:00:14,810 --> 00:00:16,180
extracts metadata,

7
00:00:16,600 --> 00:00:20,870
and creates table definitions in the AWS Glue Data Catalog.

8
00:00:22,110 --> 00:00:28,230
An AWS Glue Data Catalog is the persistent metadata store in AWS Glue.

9
00:00:29,420 --> 00:00:31,420
It contains table definitions,

10
00:00:31,430 --> 00:00:34,840
job definitions, and other control information

11
00:00:34,850 --> 00:00:37,250
to manage your AWS Glue environment.

12
00:00:38,660 --> 00:00:41,830
Actual data is not stored in the data catalog.

13
00:00:43,790 --> 00:00:45,190
When a crawler runs,

14
00:00:45,310 --> 00:00:46,660
it uses a classifier.

15
00:00:47,190 --> 00:00:48,200
A classifier

16
00:00:48,620 --> 00:00:49,820
reads the data,

17
00:00:50,710 --> 00:00:51,990
determines the format,

18
00:00:53,220 --> 00:00:54,690
and generates a schema.

19
00:00:56,410 --> 00:01:00,270
AWS Glue provides a set of built-in classifiers

20
00:01:00,550 --> 00:01:04,209
for common file types such as CSV, JSON,

21
00:01:04,489 --> 00:01:05,230
Parquet,

22
00:01:05,370 --> 00:01:05,990
AVRO,

23
00:01:06,420 --> 00:01:08,340
XML, and others.

24
00:01:09,390 --> 00:01:10,720
You can also create

25
00:01:10,840 --> 00:01:12,290
custom classifiers.

26
00:01:13,990 --> 00:01:14,590
Now,

27
00:01:14,700 --> 00:01:15,750
let's look at how

28
00:01:15,870 --> 00:01:16,910
a crawler works.

29
00:01:18,450 --> 00:01:20,620
First, a crawler progresses through

30
00:01:20,740 --> 00:01:22,880
a prioritized list of classifiers.

31
00:01:24,110 --> 00:01:26,310
If a custom classifier is defined,

32
00:01:26,760 --> 00:01:28,390
the first custom classifier

33
00:01:28,520 --> 00:01:30,320
that successfully recognizes

34
00:01:30,450 --> 00:01:31,820
the structure of your data

35
00:01:31,950 --> 00:01:33,500
is used to create a schema.

36
00:01:36,160 --> 00:01:38,070
If there is no custom classifier,

37
00:01:38,400 --> 00:01:40,770
built-in classifiers are used.

38
00:01:44,210 --> 00:01:44,920
The crawler

39
00:01:45,090 --> 00:01:46,650
then connects to the datastore.

40
00:01:47,960 --> 00:01:48,650
A crawler

41
00:01:48,790 --> 00:01:51,640
can crawl multiple datastores in a single run.

42
00:01:53,080 --> 00:01:54,050
For example,

43
00:01:54,500 --> 00:01:56,680
a crawler can crawl datastores

44
00:01:57,090 --> 00:02:01,110
such as Amazon Simple Storage Service, or Amazon S3,

45
00:02:01,570 --> 00:02:03,170
Amazon DynamoDB,

46
00:02:03,620 --> 00:02:05,180
and Delta Lake natively.

47
00:02:06,490 --> 00:02:09,880
For Java database connectivity, or JDBC,

48
00:02:10,289 --> 00:02:11,420
MongoDB,

49
00:02:11,860 --> 00:02:16,500
and Amazon DocumentDB (with MongoDB compatibility) datastores,

50
00:02:16,840 --> 00:02:17,570
a crawler

51
00:02:17,710 --> 00:02:19,970
must use an AWS Glue connection

52
00:02:20,400 --> 00:02:21,980
to connect to the datastore.

53
00:02:26,230 --> 00:02:27,410
You can run the crawler

54
00:02:27,590 --> 00:02:29,690
either on demand or on a schedule.

55
00:02:31,250 --> 00:02:32,560
Upon completion,

56
00:02:33,000 --> 00:02:35,670
the crawler then creates or updates one or more

57
00:02:35,670 --> 00:02:39,010
tables with table definitions in your data catalog.

58
00:02:40,340 --> 00:02:42,580
The tables are written to a database,

59
00:02:42,680 --> 00:02:46,200
which is a container of tables in the data catalog.

60
00:02:47,610 --> 00:02:51,090
After table definitions are added to the data catalog,

61
00:02:51,220 --> 00:02:53,830
they are available for extract, transform,

62
00:02:54,010 --> 00:02:55,200
and load, or

63
00:02:55,240 --> 00:02:56,280
ETL, jobs.

64
00:02:58,000 --> 00:03:01,110
The table definitions are also readily available

65
00:03:01,240 --> 00:03:03,320
for querying in Amazon Athena,

66
00:03:03,460 --> 00:03:04,730
Amazon EMR,

67
00:03:05,160 --> 00:03:07,140
and Amazon Redshift Spectrum.

68
00:03:14,640 --> 00:03:16,640
A crawler can run more than once.

69
00:03:17,060 --> 00:03:21,210
For example, here is the output of a crawler after running the first time.

70
00:03:23,370 --> 00:03:26,060
If your crawler runs again, perhaps on a schedule,

71
00:03:26,500 --> 00:03:30,200
it looks for new or changed files or tables in your datastore.

72
00:03:34,710 --> 00:03:36,040
The output of the crawler

73
00:03:36,490 --> 00:03:40,070
includes new tables and partitions found since the previous run.

