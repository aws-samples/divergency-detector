1
00:00:00,000 --> 00:00:02,800
Unsupervised Learning Algorithms.

2
00:00:02,820 --> 00:00:07,800
Unsupervised learning uses a range of algorithms, and here are some examples.

3
00:00:07,820 --> 00:00:09,450
Hierarchical clustering.

4
00:00:09,780 --> 00:00:13,780
Hierarchical clustering is based on a cluster tree.

5
00:00:13,920 --> 00:00:19,110
In this algorithm, each data point is considered as an individual cluster to start with.

6
00:00:24,600 --> 00:00:28,470
At each iteration, similar clusters merge with other clusters

7
00:00:41,630 --> 00:00:44,050
until a desired number of clusters are formed.

8
00:00:44,640 --> 00:00:46,120
Hierarchical clustering

9
00:00:46,250 --> 00:00:48,990
is widely used for analyzing social network data.

10
00:00:50,500 --> 00:00:51,980
K-means clustering.

11
00:00:52,570 --> 00:00:56,280
When using the k-means algorithm, you specify an input k

12
00:00:56,300 --> 00:00:59,310
that specifies the number of clusters to find in the data.

13
00:01:00,000 --> 00:01:01,280
The centroid of a plane

14
00:01:01,430 --> 00:01:04,040
is the arithmetic mean position of all the points.

15
00:01:04,910 --> 00:01:05,950
In this example,

16
00:01:06,070 --> 00:01:07,710
we choose k equals to two

17
00:01:07,860 --> 00:01:10,970
and start to place the two centroids in random locations.

18
00:01:14,060 --> 00:01:15,490
The k-means algorithm

19
00:01:15,620 --> 00:01:17,950
then iteratively determines the centroids

20
00:01:22,830 --> 00:01:25,710
and assigns each member to the closest centroid.

21
00:01:29,480 --> 00:01:33,860
Members nearest to the same centroid belong to the same group.

22
00:01:33,940 --> 00:01:38,240
Eventually, the algorithm reaches the best centroids for the k clusters.

23
00:01:42,920 --> 00:01:46,820
Common applications include market segmentation and image compression.

24
00:01:47,980 --> 00:01:50,850
Principal component analysis, or PCA.

25
00:01:52,760 --> 00:01:56,200
Principal component analysis, or PCA, algorithm

26
00:01:56,590 --> 00:02:00,360
reduces the dimensionality within a dataset by projecting

27
00:02:00,360 --> 00:02:03,490
data points onto the first few principal components.

28
00:02:04,190 --> 00:02:05,590
Principal components

29
00:02:06,030 --> 00:02:08,310
are new variables that are constructed as

30
00:02:08,310 --> 00:02:10,800
linear combinations of the initial variables

31
00:02:10,820 --> 00:02:13,030
with the help of orthogonal transformation.

32
00:02:14,130 --> 00:02:16,660
The objective is to reduce the dimensions

33
00:02:16,800 --> 00:02:20,280
while retaining as much information or variation as possible.

34
00:02:21,410 --> 00:02:22,770
In this example,

35
00:02:23,070 --> 00:02:27,520
the two-dimensional datasets are reduced to a one-dimensional representation,

36
00:02:27,720 --> 00:02:30,100
which is a function of the principal component.

37
00:02:31,730 --> 00:02:34,750
Common applications for PCA algorithm

38
00:02:34,860 --> 00:02:37,750
include facial recognition and computer vision.

