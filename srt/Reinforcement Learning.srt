1
00:00:00,820 --> 00:00:02,230
Reinforcement Learning.

2
00:00:03,080 --> 00:00:06,910
Unlike supervised and unsupervised machine-learning algorithms,

3
00:00:07,030 --> 00:00:10,050
reinforcement learning does not use initial data to train.

4
00:00:10,510 --> 00:00:13,240
The learning model is given an objective and definitions

5
00:00:13,240 --> 00:00:15,040
for the successful achievement of the goal.

6
00:00:15,480 --> 00:00:18,580
The model then iterates through actions, and through trial and error,

7
00:00:18,720 --> 00:00:20,510
learns how to best achieve the goal.

8
00:00:20,960 --> 00:00:22,620
The model consists of an agent

9
00:00:22,750 --> 00:00:23,810
computer program

10
00:00:23,940 --> 00:00:27,160
performing actions which are based on defined success criteria.

11
00:00:27,590 --> 00:00:30,470
The agent is rewarded for correct steps and penalized

12
00:00:30,470 --> 00:00:33,630
for incorrect steps until eventually an end state is reached.

13
00:00:34,060 --> 00:00:38,390
Several use cases include manufacturing automation, stock market trading,

14
00:00:38,550 --> 00:00:40,830
robotics, and self-driving cars.

15
00:00:42,410 --> 00:00:43,350
For example,

16
00:00:43,470 --> 00:00:46,600
let's train a model car to navigate to the finish line of a track.

17
00:00:47,020 --> 00:00:47,960
The model car,

18
00:00:48,120 --> 00:00:49,920
the agent, sits on the track,

19
00:00:50,080 --> 00:00:50,970
the environment.

20
00:00:51,370 --> 00:00:53,340
Its action choices are to move forward,

21
00:00:53,490 --> 00:00:54,800
turn left, or turn right.

22
00:00:55,250 --> 00:00:57,450
The car monitors what it can see of the track,

23
00:00:57,600 --> 00:00:58,510
the observation,

24
00:00:58,680 --> 00:01:01,020
and chooses to move forward, the action.

25
00:01:01,300 --> 00:01:04,400
This leads the car to a position on the next section of track.

26
00:01:04,900 --> 00:01:07,810
The car determines if it is still on the track, its state,

27
00:01:08,000 --> 00:01:10,390
and is given positive or negative reinforcement,

28
00:01:10,540 --> 00:01:11,280
the reward.

29
00:01:11,730 --> 00:01:12,890
For each new state,

30
00:01:13,040 --> 00:01:16,440
the agent must observe and act until it crosses the finish line.

31
00:01:16,900 --> 00:01:19,310
As the agent has been positively rewarded,

32
00:01:19,460 --> 00:01:22,990
it is incentivized to continue that action during that specific state.

33
00:01:24,160 --> 00:01:27,550
The agent continues this process for the duration of an episode.

34
00:01:27,720 --> 00:01:31,190
An episode being a collection of environment, state, action,

35
00:01:31,360 --> 00:01:34,050
reward, and observation information that occurs between

36
00:01:34,050 --> 00:01:36,180
a start state and a termination state.

37
00:01:36,600 --> 00:01:39,630
This methodology is called the Markov decision process.

38
00:01:40,160 --> 00:01:42,910
Compared to supervised learning, where existing labeled

39
00:01:42,910 --> 00:01:44,930
datasets are used to predict outcomes,

40
00:01:45,040 --> 00:01:45,860
for example,

41
00:01:45,970 --> 00:01:49,680
training a model to detect automobiles, and unsupervised learning, where

42
00:01:49,680 --> 00:01:53,420
unlabeled data with similar characteristics is clustered based on patterns,

43
00:01:53,550 --> 00:01:54,330
for example,

44
00:01:54,480 --> 00:01:57,540
detecting shopping patterns and offering product suggestions,

45
00:01:57,660 --> 00:02:01,170
reinforcement learning requires no initial data input or labeling,

46
00:02:01,350 --> 00:02:03,720
making setup easier and less time-consuming.

47
00:02:04,090 --> 00:02:04,800
Also,

48
00:02:04,930 --> 00:02:06,590
since there is no initial data,

49
00:02:07,010 --> 00:02:09,030
reinforcement learning tends to be less

50
00:02:09,030 --> 00:02:12,210
susceptible to bias and less computationally complex.

51
00:02:14,360 --> 00:02:18,040
Amazon SageMaker RL builds on top of Amazon SageMaker,

52
00:02:18,180 --> 00:02:20,700
adding prepackaged reinforcement learning toolkits

53
00:02:20,700 --> 00:02:22,260
and making it easy to integrate

54
00:02:22,270 --> 00:02:23,850
any simulation environment.

55
00:02:24,170 --> 00:02:27,000
Training and prediction infrastructure is fully managed,

56
00:02:27,080 --> 00:02:31,010
so that you can focus on your RL problem and not on managing servers.

57
00:02:31,410 --> 00:02:34,380
Users can deploy containers provided by SageMaker

58
00:02:34,400 --> 00:02:38,610
for Apache MXNet and TensorFlow or create their own custom environments.

59
00:02:39,490 --> 00:02:44,050
Some popular algorithms used in RL include Monte Carlo, Q-learning,

60
00:02:44,080 --> 00:02:47,680
DQN, and proximal policy optimization, or PPO.

